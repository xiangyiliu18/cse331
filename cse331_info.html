<!DOCTYPE html>
<html lang="en">
<head>
<title>CSS Template</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<body>
<div class="container">
<h3>1.Scrapy setting</h3>
<div style="margin-left: 5%">
<h4> 1.1 Default Settings </h4>
<p> <pre>
Default: SCHEDULER_DISK_QUEUE ='scrapy.squeues.PickleLifoDiskQueue'

Default: SCHEDULER_MEMORY_QUEUE ='scrapy.squeues.LifoMemoryQueue'

Default: DEPTH_PRIORITY = 0  ## If zero, no limit will be imposed.

Default:CLOSESPIDER_PAGECOUNT = 0 

Default: USER_AGENT = "Scrapy/VERSION (+https://scrapy.org)"</pre></p>
<h4>1.2 Change to BFO order and Change other values :</h4>
<div class="alert alert-warning" role="alert">
 Spiders can receive arguments that modify their behaviour. Some common uses for spider arguments are to define the start URLs or to restrict the crawl to certain sections of the site, but they can be used to configure any functionality of the spider. <span style="color; black;"><a href="https://doc.scrapy.org/en/latest/topics/spiders.html#spiderargs"> see details in here</a> </span>
</div>
<p>
<pre>
DEPTH_PRIORITY = 1 

SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'

SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue
</pre></p>
</h5>
</div>
<hr/>

<h3> 2. Handle "dead-end pages" example:</h3>
	<a class="btn btn-success" style="margin-left: 5%;" href="https://akashjaindxb.com/2016/03/29/crawl-your-site-and-find-all-broken-links-using-scrapy/">click the link here
</a></body>
<hr/>
<div>
<h3> 3. Robot sources </h3>
<a class="btn btn-success" style="margin-left: 5%;" href="http://www.robotstxt.org/db.html">click the link to see all Robot DB
</a><br/><br/>
<a class="btn btn-success" style="margin-left: 5%;" href="http://www.robotstxt.org/robotstxt.html">click the link to see robox.txt example
</a><br/><br/>

<a class="btn btn-success" style="margin-left: 5%;"  href="https://blog.scrapinghub.com/2016/08/25/how-to-crawl-the-web-politely-with-scrapy">More example</a>
<br/><br/>

<a class="btn btn-success" style="margin-left: 5%;"  href="https://doc.scrapy.org/en/latest/topics/settings.html#robotstxt-obey">Connect Scrapy with robot.txt</a>
<br/><br/>
<img src="./robot_instruc.PNG" class="img-thumbnail" alt="Responsive image"  width="600" height="600" >
</div>

<div>
	<h3>4. Python's Brute Force</h3>
	<a class="btn btn-success" style="margin-left: 5%;"  href="https://www.bushisecurity.com/2017/09/04/python-ftp-brute-force/">Click here to see example</a>
</div>


<div>
	<h3>4. HTTP Request/ Get-Post</h3>
	<a class="btn btn-success" style="margin-left: 5%;"  href="https://programminghistorian.org/en/lessons/creating-apis-with-python-and-flask/">Example1</a><br/><br/>
		<a class="btn btn-success" style="margin-left: 5%;"  href="https://medium.com/@Saslow/building-a-flask-web-application-a66acafea2d2">Example2</a><br/><br/>
</div>
<div>
<h3>5 : Begin </h3>
<div class="alert alert-warning" role="alert">
<pre>
	ec2-18-216-197-130.us-east-2.compute.amazonaws.com
useranem: user
password:EN9fQcC3jnsS
	#1- Has one words_list.txt
	#2- Has retrieved the login HTML pages
	#3- Parse the resulting HTML looking for username and password field as part of the input form
	#4- Performs a POST on the login page with the username and the retrieved password
	#5- Retrieve the resulting HTML page. If the page does not have the login form,
	#we assume Brute-Force is successful. Otherwise, repeat the whole process with
	#the next password in the 
</pre>
</div>
</div>
</div>
</html>
